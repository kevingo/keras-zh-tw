
## activation function 的使用方法

Activation 可以透過 `Activation` layer，或透過任何支援 `activtion` 參數的layer：

```python
from keras.layers import Activation, Dense

model.add(Dense(64))
model.add(Activation('tanh'))
```

上面的寫法等同於：

```python
model.add(Dense(64, activation='tanh'))
```

你也可以透過 TensorFlow/Theano/CNTK 等框架來引入 Activation function：

```python
from keras import backend as K

model.add(Dense(64, activation=K.tanh))
model.add(Activation(K.tanh))
```

## 可用的 activations

{{autogenerated}}

## On "Advanced Activations"

Activations that are more complex than a simple TensorFlow/Theano/CNTK function (eg. learnable activations, which maintain a state) are available as [Advanced Activation layers](layers/advanced-activations.md), and can be found in the module `keras.layers.advanced_activations`. These include `PReLU` and `LeakyReLU`.

Activation 函式你想像的還要複雜(例如：可被學習的 activation，用來保存某種狀態)，可以參考 [進階 activation layer 章節](layers/advanced-activations.md)，或是可以參考 `keras.layers.advanced_activations`，當中會包含 `PReLU` 和 `LeakyReLU`。